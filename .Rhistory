currentDate <- Sys.Date()
#attr(currentDate, "tzone") <- "UTC" #doesn't work
yesterday <- currentDate -1
userTimeline(user,n=10)
userTimeline(ConnerShoop,n=10)
userTimeline("ConnerShoop",n=10)
userTimeline("TheRealDonaldTrump",n=10)
userTimeline("realDonaldTrump",n=10)
userTimeline("ConnerShoop",sinceID = currentDate-500)
myTweets <- userTimeline("ConnerShoop",sinceID = currentDate-500)
View(myTweets)
myTweets <- twListToDF(myTweets)
View(myTweets)
myTweets <- twListToDF(myTweets)
myTweets <- twListToDF(myTweets)
doc_article <- data_frame(document = myTweets$screenName, article = myTweets$text)
View(doc_article)
View(event_data)
doc_article <- data_frame(document = myTweets$screenName, article = myTweets$text, time=myTweets$start_time)
View(doc_article)
View(event_data)
myTweets <- twListToDF(myTweets)
View(myTweets)
doc_article <- data_frame(document = myTweets$screenName, article = myTweets$text, time=myTweets$created)
View(doc_article)
#make stop words
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#make stop words
custom_stop_words <- c("dog")
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
View(doc_term)
View(doc_term)
#unnewst @ and other symbols
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
doc_term <- doc_term %>%
unnest_tokens(word, word, token = "regex", pattern = unnest_reg)
View(doc_term)
View(doc_term)
#unnest terms with 0-9 in them
toMatch <- c("0","1", "2", "3","4", "5", "6","7", "8", "9")
matches <- as.data.frame(grep(paste(toMatch,collapse="|"),
doc_term$word, value=TRUE))
colnames(matches) <- c("word")
doc_term %>%
anti_join(matches)
View(doc_term)
#unnest terms with 0-9 in them
toMatch <- c("0","1", "2", "3","4", "5", "6","7", "8", "9")
matches <- as.data.frame(grep(paste(toMatch,collapse="|"),
doc_term$word, value=TRUE))
colnames(matches) <- c("word")
View(matches)
doc_term %>%
anti_join(matches)
View(matches)
View(doc_term)
doc_term <- doc_term %>%
anti_join(matches)
View(doc_term)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
#make stop words
custom_stop_words <- c("https", "t")
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
View(doc_term)
#wordcloud
counter <- doc_term %>%
group_by(document) %>%
count(word)
View(counter)
wordcloud(counter$word, counter$n, max.words = 100)
wordcloud(counter$word, counter$n, max.words = 5)
#make stop words
custom_stop_words <- c("https", "t","t.co")
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
View(myTweets)
View(doc_term)
View(counter)
myTweets <- userTimeline("realDonaldTrump",sinceID = currentDate-500)
myTweets <- twListToDF(myTweets)
View(myTweets)
myTweets <- userTimeline("realDonaldTrump",sinceID = currentDate-5000)
myTweets <- twListToDF(myTweets)
myTweets <- userTimeline("realDonaldTrump",n=3200, sinceID = currentDate-5000)
myTweets <- twListToDF(myTweets)
doc_article <- data_frame(document = myTweets$screenName, article = myTweets$text, time=myTweets$created)
#make stop words
custom_stop_words <- c("https", "t","t.co")
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
#unnest @ and other symbols
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
doc_term <- doc_term %>%
unnest_tokens(word, word, token = "regex", pattern = unnest_reg)
#unnest terms with 0-9 in them
toMatch <- c("0","1", "2", "3","4", "5", "6","7", "8", "9")
matches <- as.data.frame(grep(paste(toMatch,collapse="|"),
doc_term$word, value=TRUE))
colnames(matches) <- c("word")
doc_term <- doc_term %>%
anti_join(matches)
#wordcloud
counter <- doc_term %>%
group_by(document) %>%
count(word)
wordcloud(counter$word, counter$n, max.words = 5)
wordcloud(counter$word, counter$n, max.words = 100)
wordcloud(counter$word, counter$n, max.words = 50)
wordcloud(counter$word, counter$n, max.words = 50)
#setup with authentication key for twitter API
consumer_key <- 'eZPpCGjwmbZTV8hH0BIAk74pv'
consumer_secret <- 'PLRTEqmDo2Tog32uvpUFL72P64TNR2BLfKepgefcQLJlI2jgig'
access_token <- '915228318173102081-3iAy2xUbmZVm8MREvXxqgp1o7WHF5h4'
access_secret <- 'bgqJKoZhUXOAso2od4PuzirrmZodOsGRhLj8geq8fMwA2'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#get current date and yesterday
#need to correct for difference in date for UTC still
currentDate <- Sys.Date()
#attr(currentDate, "tzone") <- "UTC" #doesn't work
yesterday <- currentDate -1
myTweets <- userTimeline("iunlearn",n=3200, sinceID = currentDate-5000)
myTweets <- twListToDF(myTweets)
doc_article <- data_frame(document = myTweets$screenName, article = myTweets$text, time=myTweets$created)
#make stop words
custom_stop_words <- c("https", "t","t.co")
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
#unnest @ and other symbols
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
doc_term <- doc_term %>%
unnest_tokens(word, word, token = "regex", pattern = unnest_reg)
#unnest terms with 0-9 in them
toMatch <- c("0","1", "2", "3","4", "5", "6","7", "8", "9")
matches <- as.data.frame(grep(paste(toMatch,collapse="|"),
doc_term$word, value=TRUE))
colnames(matches) <- c("word")
doc_term <- doc_term %>%
anti_join(matches)
#wordcloud
counter <- doc_term %>%
group_by(document) %>%
count(word)
counter <- counter[grep("Grateful Dead", counter$document), ]
wordcloud(counter$word, counter$n, max.words = 50)
wordcloud(counter$word, counter$n, max.words = 50)
#wordcloud
counter <- doc_term %>%
group_by(document) %>%
count(word)
wordcloud(counter$word, counter$n, max.words = 50)
View(counter)
myTweets <- userTimeline("thejhp",n=3200, sinceID = currentDate-5000)
myTweets <- twListToDF(myTweets)
doc_article <- data_frame(document = myTweets$screenName, article = myTweets$text, time=myTweets$created)
View(doc_article)
#make stop words
custom_stop_words <- c("https", "t","t.co")
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
#unnest @ and other symbols
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
#wordcloud
counter <- doc_term %>%
group_by(document) %>%
count(word)
wordcloud(counter$word, counter$n, max.words = 50)
#unnest @ and other symbols
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
doc_term <- doc_term %>%
unnest_tokens(word, word, token = "regex", pattern = unnest_reg)
#wordcloud
counter <- doc_term %>%
group_by(document) %>%
count(word)
wordcloud(counter$word, counter$n, max.words = 50)
#search twitter function from TwitteR makes tweets into data frame of n 16 dimension objects
#starts at latest time in time frame, and searches backwards in time
#until it finds n objects(tweets)
data <- searchTwitter('#soc119', since = toString(yesterday),
until = toString(currentDate), n=n)
#search twitter function from TwitteR makes tweets into data frame of n 16 dimension objects
#starts at latest time in time frame, and searches backwards in time
#until it finds n objects(tweets)
data <- searchTwitter('soc119', since = toString(yesterday),
until = toString(currentDate), n=n)
#search twitter function from TwitteR makes tweets into data frame of n 16 dimension objects
#starts at latest time in time frame, and searches backwards in time
#until it finds n objects(tweets)
data <- searchTwitter('#soc119', since = toString(yesterday),
until = toString(currentDate), n=1000)
data <- twListToDF(data)
View(data)
#search twitter function from TwitteR makes tweets into data frame of n 16 dimension objects
#starts at latest time in time frame, and searches backwards in time
#until it finds n objects(tweets)
data <- searchTwitter('#soc119', since = toString(yesterday-6),
until = toString(currentDate), n=1000)
data <- twListToDF(data)
View(data)
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Documents/GitHub/connershoop.github.io")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Documents/GitHub/connershoop.github.io")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Documents/GitHub/connershoop.github.io")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Documents/GitHub/connershoop.github.io")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Documents/GitHub/connershoop.github.io")
#render your sweet site.
rmarkdown::render_site()
#setup with authentication key for twitter API
consumer_key <- 'eZPpCGjwmbZTV8hH0BIAk74pv'
consumer_secret <- 'PLRTEqmDo2Tog32uvpUFL72P64TNR2BLfKepgefcQLJlI2jgig'
access_token <- '915228318173102081-3iAy2xUbmZVm8MREvXxqgp1o7WHF5h4'
access_secret <- 'bgqJKoZhUXOAso2od4PuzirrmZodOsGRhLj8geq8fMwA2'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
library(twitteR)
library(wordcloud)
library(RColorBrewer)
library(POSIX)
library(ggplot2)
library(tm)
library(SnowballC)
library(stringr)
library(twitteR)
library(wordcloud)
library(RColorBrewer)
library(POSIX)
library(ggplot2)
library(tm)
#setup with authentication key for twitter API
consumer_key <- 'eZPpCGjwmbZTV8hH0BIAk74pv'
consumer_secret <- 'PLRTEqmDo2Tog32uvpUFL72P64TNR2BLfKepgefcQLJlI2jgig'
access_token <- '915228318173102081-3iAy2xUbmZVm8MREvXxqgp1o7WHF5h4'
access_secret <- 'bgqJKoZhUXOAso2od4PuzirrmZodOsGRhLj8geq8fMwA2'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#get current date and yesterday
#need to correct for difference in date for UTC still
currentDate  <- Sys.Date()
#attr(currentDate, "tzone") <- "UTC" #doesn't work
yesterday <- currentDate -1
#make any date
date <- as.Date("2018-01-29")
####### ####### ####### ####### ####### ####### ####### #######
#get tweets by keyword
myTweets <- searchTwitter('#soc119 -filter:retweets', since = toString(date),
until = toString(date+7) ,n=9999)
myTweets <- twListToDF(myTweets)
####### ####### ####### ####### ####### ####### ####### #######
####### BASIC FIRST MANIPULATION OF DATA FRAME
#reduce data frame and convert time to POSIXct object
doc_article <- data_frame(document = myTweets$screenName, article = myTweets$text, time=myTweets$created)
doc_article$time <- as.POSIXct(doc_article$time)
#make stop words
custom_stop_words <- c("https", "t","t.co", "soc119","it's", "people")
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
#make lower case
doc_term$word <- tolower(doc_term$word)
####### ####### ####### ####### ####### ####### ####### #######
####### GETTING RID OF SYMBOLS AND NUMBERS
#unnest @ and other symbols
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
doc_term <- doc_term %>%
unnest_tokens(word, word, token = "regex", pattern = unnest_reg)
#unnest terms with 0-9 in them
toMatch <- c("0","1", "2", "3","4", "5", "6","7", "8", "9")
matches <- as.data.frame(grep(paste(toMatch,collapse="|"),
doc_term$word, value=TRUE))
colnames(matches) <- c("word")
doc_term <- doc_term %>%
anti_join(matches)
#stemWords
doc_term <- doc_term %>%
mutate(word = wordStem(word))
####### ####### ####### ####### ####### ####### ####### #######
# find time per 10 minutes that week
# vs number of tweets in that 10 minute interval
time <- "2018-01-29 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","n")
x$time <- seq(time,time+1007*10*60,
10*60)
for (i in 1:nrow) {
array1 <- doc_article$time > time+(i-1)*60*10
array2 <- doc_article$time < time+i*60*10
x[i,2] <- sum(array1*array2)
}
#plot function for that time
ggplot(x, aes(time, n), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("#soc119 tweets per every 10 minutes") +
ggtitle("freq of tweets containing #Soc119 Jan22 to Jan 29, 2018")
####### ####### ####### ####### ####### ####### ####### #######
# find time per 10 minutes that week
# vs number of tweets in that 10 minute interval
time <- "2018-01-29 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","n")
x$time <- seq(time,time+1007*10*60,
10*60)
for (i in 1:nrow) {
array1 <- doc_article$time > time+(i-1)*60*10
array2 <- doc_article$time < time+i*60*10
x[i,2] <- sum(array1*array2)
}
#plot function for that time
ggplot(x, aes(time, n), ) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("#soc119 tweets per every 10 minutes") +
ggtitle("freq of tweets containing #Soc119 Jan22 to Jan 29, 2018")
afinn <- get_sentiments('afinn')
#get doc_sentiment
doc_sentiment <- doc_term %>%
inner_join(afinn)
time <- "2018-01-29 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
for (i in 1:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)
}
#plot function for that time
ggplot(x, aes(time, sentimentTot)) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("Sentiment per 10 minutes intervals (-5 negative to 5 positive scale per word)") +
ggtitle("Sentiment of tweets containing containing #Soc119 Jan22 to Jan 29, 2018")
####### ####### ####### ####### ####### ####### ####### #######
####### find cumulativew sentiment per 10 minutes in a week
time <- "2018-01-29 00:00:00"
time <- as.POSIXct(time, tz="UTC")
nrow=1008
x <- data.frame(matrix(ncol=2,nrow=nrow))
colnames(x) <-c("time","sentimentTot")
x$time <- seq(time,time+1007*10*60,
10*60)
array1 <- doc_sentiment$time > time
array2 <- doc_sentiment$time < time+60*10
x[1,2] <- sum(array1*array2*doc_sentiment$score)
for (i in 2:nrow) {
array1 <- doc_sentiment$time > time+(i-1)*60*10
array2 <- doc_sentiment$time < time+i*60*10
x[i,2] <- sum(array1*array2*doc_sentiment$score)+x[i-1,2]
}
#plot function for that time
ggplot(x, aes(time, sentimentTot)) + geom_line() + geom_area()  +
scale_x_datetime(date_breaks = "1 days") + xlab("day") + ylab("sentiment (-5 negative to 5 positive scale per word)") +
ggtitle("Cumulative sentiment of tweets containing containing #Soc119 Jan 22 to Jan 29, 2018")
####### ####### ####### ####### ####### ####### ####### #######
#wordcloud
counter <- doc_term %>%
count(word) %>%
arrange(desc(n))
wordcloud(counter$word, counter$n, max.words = 100)
doc_term_all <- doc_article %>%
unnest_tokens(word, article) %>%
group_by(document) %>%
summarise(total=n()) %>%
arrange(desc(total))
top10 <- doc_term_all[1:10,1]
bottom10 <- doc_term_all[nrow(doc_term_all):(nrow(doc_term_all)-9),1]
View(bottom10)
top10tweets <- inner_join(top10,doc_article)
bottom10tweets <- inner_join(bottom10,doc_article)
View(bottom10)
View(top10tweets)
View(bottom10tweets)
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Documents/GitHub/connershoop.github.io")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Documents/GitHub/connershoop.github.io")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Documents/GitHub/connershoop.github.io")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Documents/GitHub/connershoop.github.io")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Documents/GitHub/connershoop.github.io")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Documents/GitHub/connershoop.github.io")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Documents/GitHub/connershoop.github.io")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Documents/GitHub/connershoop.github.io")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("~/Documents/GitHub/connershoop.github.io")
#render your sweet site.
rmarkdown::render_site()
shiny::runApp('~/Documents/RStudio/ShinyPractice')
runApp('~/Documents/RStudio/ShinyPractice')
runApp('~/Documents/RStudio/ShinyPractice')
runApp('~/Documents/RStudio/ShinyPractice')
runApp('~/Documents/RStudio/ShinyPractice')
runApp('~/Documents/RStudio/ShinyPractice')
runApp('~/Documents/RStudio/ShinyPractice')
