library(shiny)
ui <- fluidPage(
# Application title
titlePanel(h4("Music Mapp: Find Events Near You!")),
sidebarLayout(
sidebarPanel(
textInput("text", h3("Location"),
value = "Enter location..."),
checkboxInput('bar','Select All/None'),
checkboxGroupInput("checkGroup",
h3("Categories"),
choices = list("music"=1,
"conference"=2,
"comedy"=3,
"learning_education"=4,
"family_fun_kids"=5,
"festivals_parades"=6,
"movies_film"=7,
"food"=8,
"fundraisers"=9,
"art"=10,
"support"=11,
"holiday"=12,
"books"=13,
"attractions"=14,
"community"=15,
"business"=16,
"singles_social"=17,
"schools_alumni"=18,
"clubs_associations"=19,
"outdoors_recreation"=20,
"performing_arts"=21,
"animals"=22,
"politics_activism"=23,
"sales"=24,
"science"=25,
"religion_spirituality"=26,
"sports"=27,
"technology"=28,
"other"=29)),
dateRangeInput("dates", h3("Date range")),
actionButton("cast", "Search for events!", icon = icon("thumbs-up"))
),
mainPanel(textOutput("Map"))
)
)
library(shiny); runApp('Documents/GitHub/group8_project/ShinyApp.R')
#search
key <- "zhkxv5C4PzvT2TpC"
category <- "comedy"
location <- "Pittsburg"
date <- "December"
# function to search by location, date, category, key
# requires package dplyr
library(dplyr)
search_events <- function(location,date,category,key){
url <- paste('http://api.eventful.com/rest/events/search?app_key=', key, '&page_size=100&category=', category, '&location=',location,'&date=',date, sep ="")
event_data <- xmlParse(url)
event_data <- xmlToList(event_data)$events
event_data <- as.data.frame(do.call('rbind',event_data),nrow=length(event_data),ncol=length(event_data[[1]]),stringsAsFactors = FALSE)
event_data <- event_data %>%
select(title, url, description, start_time, venue_name, city_name, region_name, postal_code, latitude, longitude, image)
}
#call search function
event_data <- search_events("New York", "december", "comedy",key)
# function to search by location, date, category, key
# requires package dplyr
library(dplyr)
install.packages("dplyr")
search_events <- function(location,date,category,key){
url <- paste('http://api.eventful.com/rest/events/search?app_key=', key, '&page_size=100&category=', category, '&location=',location,'&date=',date, sep ="")
event_data <- xmlParse(url)
event_data <- xmlToList(event_data)$events
event_data <- as.data.frame(do.call('rbind',event_data),nrow=length(event_data),ncol=length(event_data[[1]]),stringsAsFactors = FALSE)
event_data <- event_data %>%
select(title, url, description, start_time, venue_name, city_name, region_name, postal_code, latitude, longitude, image)
}
#call search function
event_data <- search_events("New York", "december", "comedy",key)
install.packages("XML")
# function to search by location, date, category, key
# requires package dplyr
library(dplyr)
search_events <- function(location,date,category,key){
url <- paste('http://api.eventful.com/rest/events/search?app_key=', key, '&page_size=100&category=', category, '&location=',location,'&date=',date, sep ="")
event_data <- xmlParse(url)
event_data <- xmlToList(event_data)$events
event_data <- as.data.frame(do.call('rbind',event_data),nrow=length(event_data),ncol=length(event_data[[1]]),stringsAsFactors = FALSE)
event_data <- event_data %>%
select(title, url, description, start_time, venue_name, city_name, region_name, postal_code, latitude, longitude, image)
}
#call search function
event_data <- search_events("New York", "december", "comedy",key)
#create data frame specifically of latitude and longitude
lat_lon <- event_data %>%
select(latitude, longitude)
search_events <- function(location,date,category,key){
url <- paste('http://api.eventful.com/rest/events/search?app_key=', key, '&page_size=100&category=', category, '&location=',location,'&date=',date, sep ="")
event_data <- xmlParse(url)
event_data <- xmlToList(event_data)$events
event_data <- as.data.frame(do.call('rbind',event_data),nrow=length(event_data),ncol=length(event_data[[1]]),stringsAsFactors = FALSE)
event_data <- event_data %>%
select(title, url, description, start_time, venue_name, city_name, region_name, postal_code, latitude, longitude, image)
}
#call search function
event_data <- search_events("New York", "december", "comedy",key)
library(XML)
#call search function
event_data <- search_events("New York", "december", "comedy",key)
View(event_data)
search_events <- function(location,date,category,key){
url <- paste('http://api.eventful.com/rest/events/search?app_key=', key, '&page_size=1000&category=', category, '&location=',location,'&date=',date, sep ="")
event_data <- xmlParse(url)
event_data <- xmlToList(event_data)$events
event_data <- as.data.frame(do.call('rbind',event_data),nrow=length(event_data),ncol=length(event_data[[1]]),stringsAsFactors = FALSE)
event_data <- event_data %>%
select(title, url, description, start_time, venue_name, city_name, region_name, postal_code, latitude, longitude, image)
}
#call search function
event_data <- search_events("New York", "december", "comedy",key)
View(event_data)
# function to search by location, date, category, key
search_events <- function(location,date,category,key){
url <- paste('http://api.eventful.com/rest/events/search?app_key=', key, '&page_size=1000&category=', category, '&location=',location,'&date=',date, sep ="")
event_data <- xmlParse(url)
event_data <- xmlToList(event_data)$events
event_data <- as.data.frame(do.call('rbind',event_data),nrow=length(event_data),ncol=length(event_data[[1]]),stringsAsFactors = FALSE)
}
#call search function
event_data <- search_events("New York", "december", "comedy",key)
View(event_data)
#create data frame specifically of latitude and longitude
lat_lon <- event_data %>%
select(latitude, longitude)
library(XML)
library(shiny); runApp('Documents/GitHub/group8_project/inst/Shiny/ShinyApp.R')
runApp('Documents/GitHub/group8_project/inst/Shiny/ShinyApp.R')
devtools::install_github("musicmapp")
devtools::install_github("group8/stat297")
library(musicmapp)
run_shiny()
run_shiny()
run_shiny()
library(shiny)
library(shinydashboard)
library(leaflet)
library(XML)
run_shiny()
search_events()
?search_events
library(shiny); runApp('Documents/GitHub/group8_project/inst/Shiny/ShinyApp.R')
runApp('Documents/GitHub/group8_project/inst/Shiny/ShinyApp.R')
runApp('Documents/GitHub/group8_project/inst/Shiny/ShinyApp.R')
#requires these packages
library(readr)
library(tidytext)
library(dplyr)
library(twitteR)
library(tidyr)
library(SnowballC)
library(stringr)
library(twitteR)
library(wordcloud)
#setup with authentication key for twitter API
consumer_key <- 'eZPpCGjwmbZTV8hH0BIAk74pv'
consumer_secret <- 'PLRTEqmDo2Tog32uvpUFL72P64TNR2BLfKepgefcQLJlI2jgig'
access_token <- '915228318173102081-3iAy2xUbmZVm8MREvXxqgp1o7WHF5h4'
access_secret <- 'bgqJKoZhUXOAso2od4PuzirrmZodOsGRhLj8geq8fMwA2'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#get current date and yesterday
#need to correct for difference in date for UTC still
currentDate <- Sys.Date()
#attr(currentDate, "tzone") <- "UTC" #doesn't work
yesterday <- currentDate -1
userTimeline(user,n=10)
userTimeline(ConnerShoop,n=10)
userTimeline("ConnerShoop",n=10)
userTimeline("TheRealDonaldTrump",n=10)
userTimeline("realDonaldTrump",n=10)
userTimeline("ConnerShoop",sinceID = currentDate-500)
myTweets <- userTimeline("ConnerShoop",sinceID = currentDate-500)
View(myTweets)
myTweets <- twListToDF(myTweets)
View(myTweets)
myTweets <- twListToDF(myTweets)
myTweets <- twListToDF(myTweets)
doc_article <- data_frame(document = myTweets$screenName, article = myTweets$text)
View(doc_article)
View(event_data)
doc_article <- data_frame(document = myTweets$screenName, article = myTweets$text, time=myTweets$start_time)
View(doc_article)
View(event_data)
myTweets <- twListToDF(myTweets)
View(myTweets)
doc_article <- data_frame(document = myTweets$screenName, article = myTweets$text, time=myTweets$created)
View(doc_article)
#make stop words
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#make stop words
custom_stop_words <- c("dog")
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
View(doc_term)
View(doc_term)
#unnewst @ and other symbols
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
doc_term <- doc_term %>%
unnest_tokens(word, word, token = "regex", pattern = unnest_reg)
View(doc_term)
View(doc_term)
#unnest terms with 0-9 in them
toMatch <- c("0","1", "2", "3","4", "5", "6","7", "8", "9")
matches <- as.data.frame(grep(paste(toMatch,collapse="|"),
doc_term$word, value=TRUE))
colnames(matches) <- c("word")
doc_term %>%
anti_join(matches)
View(doc_term)
#unnest terms with 0-9 in them
toMatch <- c("0","1", "2", "3","4", "5", "6","7", "8", "9")
matches <- as.data.frame(grep(paste(toMatch,collapse="|"),
doc_term$word, value=TRUE))
colnames(matches) <- c("word")
View(matches)
doc_term %>%
anti_join(matches)
View(matches)
View(doc_term)
doc_term <- doc_term %>%
anti_join(matches)
View(doc_term)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
#make stop words
custom_stop_words <- c("https", "t")
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
View(doc_term)
#wordcloud
counter <- doc_term %>%
group_by(document) %>%
count(word)
View(counter)
wordcloud(counter$word, counter$n, max.words = 100)
wordcloud(counter$word, counter$n, max.words = 5)
#make stop words
custom_stop_words <- c("https", "t","t.co")
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
View(myTweets)
View(doc_term)
View(counter)
myTweets <- userTimeline("realDonaldTrump",sinceID = currentDate-500)
myTweets <- twListToDF(myTweets)
View(myTweets)
myTweets <- userTimeline("realDonaldTrump",sinceID = currentDate-5000)
myTweets <- twListToDF(myTweets)
myTweets <- userTimeline("realDonaldTrump",n=3200, sinceID = currentDate-5000)
myTweets <- twListToDF(myTweets)
doc_article <- data_frame(document = myTweets$screenName, article = myTweets$text, time=myTweets$created)
#make stop words
custom_stop_words <- c("https", "t","t.co")
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
#unnest @ and other symbols
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
doc_term <- doc_term %>%
unnest_tokens(word, word, token = "regex", pattern = unnest_reg)
#unnest terms with 0-9 in them
toMatch <- c("0","1", "2", "3","4", "5", "6","7", "8", "9")
matches <- as.data.frame(grep(paste(toMatch,collapse="|"),
doc_term$word, value=TRUE))
colnames(matches) <- c("word")
doc_term <- doc_term %>%
anti_join(matches)
#wordcloud
counter <- doc_term %>%
group_by(document) %>%
count(word)
wordcloud(counter$word, counter$n, max.words = 5)
wordcloud(counter$word, counter$n, max.words = 100)
wordcloud(counter$word, counter$n, max.words = 50)
wordcloud(counter$word, counter$n, max.words = 50)
#setup with authentication key for twitter API
consumer_key <- 'eZPpCGjwmbZTV8hH0BIAk74pv'
consumer_secret <- 'PLRTEqmDo2Tog32uvpUFL72P64TNR2BLfKepgefcQLJlI2jgig'
access_token <- '915228318173102081-3iAy2xUbmZVm8MREvXxqgp1o7WHF5h4'
access_secret <- 'bgqJKoZhUXOAso2od4PuzirrmZodOsGRhLj8geq8fMwA2'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#get current date and yesterday
#need to correct for difference in date for UTC still
currentDate <- Sys.Date()
#attr(currentDate, "tzone") <- "UTC" #doesn't work
yesterday <- currentDate -1
myTweets <- userTimeline("iunlearn",n=3200, sinceID = currentDate-5000)
myTweets <- twListToDF(myTweets)
doc_article <- data_frame(document = myTweets$screenName, article = myTweets$text, time=myTweets$created)
#make stop words
custom_stop_words <- c("https", "t","t.co")
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
#unnest @ and other symbols
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
doc_term <- doc_term %>%
unnest_tokens(word, word, token = "regex", pattern = unnest_reg)
#unnest terms with 0-9 in them
toMatch <- c("0","1", "2", "3","4", "5", "6","7", "8", "9")
matches <- as.data.frame(grep(paste(toMatch,collapse="|"),
doc_term$word, value=TRUE))
colnames(matches) <- c("word")
doc_term <- doc_term %>%
anti_join(matches)
#wordcloud
counter <- doc_term %>%
group_by(document) %>%
count(word)
counter <- counter[grep("Grateful Dead", counter$document), ]
wordcloud(counter$word, counter$n, max.words = 50)
wordcloud(counter$word, counter$n, max.words = 50)
#wordcloud
counter <- doc_term %>%
group_by(document) %>%
count(word)
wordcloud(counter$word, counter$n, max.words = 50)
View(counter)
myTweets <- userTimeline("thejhp",n=3200, sinceID = currentDate-5000)
myTweets <- twListToDF(myTweets)
doc_article <- data_frame(document = myTweets$screenName, article = myTweets$text, time=myTweets$created)
View(doc_article)
#make stop words
custom_stop_words <- c("https", "t","t.co")
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,
lexicon = c("custom")),
stop_words)
#unnest article and antijoin with stop words
doc_term <- doc_article %>%
unnest_tokens(word, article)  %>%
anti_join(custom_stop_words)
#unnest @ and other symbols
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
#wordcloud
counter <- doc_term %>%
group_by(document) %>%
count(word)
wordcloud(counter$word, counter$n, max.words = 50)
#unnest @ and other symbols
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
doc_term <- doc_term %>%
unnest_tokens(word, word, token = "regex", pattern = unnest_reg)
#wordcloud
counter <- doc_term %>%
group_by(document) %>%
count(word)
wordcloud(counter$word, counter$n, max.words = 50)
#search twitter function from TwitteR makes tweets into data frame of n 16 dimension objects
#starts at latest time in time frame, and searches backwards in time
#until it finds n objects(tweets)
data <- searchTwitter('#soc119', since = toString(yesterday),
until = toString(currentDate), n=n)
#search twitter function from TwitteR makes tweets into data frame of n 16 dimension objects
#starts at latest time in time frame, and searches backwards in time
#until it finds n objects(tweets)
data <- searchTwitter('soc119', since = toString(yesterday),
until = toString(currentDate), n=n)
#search twitter function from TwitteR makes tweets into data frame of n 16 dimension objects
#starts at latest time in time frame, and searches backwards in time
#until it finds n objects(tweets)
data <- searchTwitter('#soc119', since = toString(yesterday),
until = toString(currentDate), n=1000)
data <- twListToDF(data)
View(data)
#search twitter function from TwitteR makes tweets into data frame of n 16 dimension objects
#starts at latest time in time frame, and searches backwards in time
#until it finds n objects(tweets)
data <- searchTwitter('#soc119', since = toString(yesterday-6),
until = toString(currentDate), n=1000)
data <- twListToDF(data)
View(data)
rmarkdown::render_site()
setwd("~/Documents/GitHub/connershoop.github.io")
setwd("~/Documents/GitHub/connershoop.github.io")
