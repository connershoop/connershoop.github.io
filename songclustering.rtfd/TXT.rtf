{\rtf1\ansi\ansicpg1252\cocoartf1671
{\fonttbl\f0\froman\fcharset0 Times-Roman;\f1\froman\fcharset0 Times-Bold;\f2\fmodern\fcharset0 Courier;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue0;\red255\green255\blue255;
\red0\green0\blue233;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c0\c0\c0\c3922;\cssrgb\c100000\c100000\c100000;
\cssrgb\c0\c0\c93333;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
{\info
{\title Song Lyrics used to Cluster Artists}}\vieww10800\viewh8400\viewkind0
\deftab720
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl280\partightenfactor0
\ls1\ilvl0
\f0\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\pard\pardeftab720\sl900\sa509\partightenfactor0

\f1\b\fs76 \cf2 Song Lyrics used to Cluster Artists\
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0\b0\fs24 \cf2 In this case study we examine the lyrics of top songs by popular artists, which is contained in a datafile found on Kaggle. We try and group these artists into separate group. We create a term frequency inverse document matrix, which turns out to work the best for our application.\
After reading this case study I would highly recommend reading \'93Text Mining in R\'94. This is available online for free. It gives an in depth overview of many of the methods I will be using.\uc0\u8232 First, we will need some packages installed\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 library(readr)\
library(tidytext)\
library(dplyr)\
library(stringr)\
library(tidyr)\
library(SnowballC)\
library(heatmaply)\
library(wordcloud)\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 So, first we read our csv songdata file and make a data frame with columns artists and lyrics. In our data frame doc_article we name the artist column \'93document\'94, and song column \'93article\'94. We call artists document because it keeps our code general in the case it one day finds other use. Just remember, for this example document = artist.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 data <- read_csv("songdata.csv")\
doc_article <- data_frame(document = data[[1]], article = data[[4]])\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 There are some words we wish to be ignored in our analysis, in the tidyversethese are called stop words and they include words like \'93the\'94 and \'93and\'94 etc. They are called stop_words. We write some code to add our own stop words for this song data, \'93chorus\'94 and \'93verse\'94, to the stop words stop_words, from the tidyverse.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 custom_stop_words <- c("chorus", "verse")\
custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,\
                                            lexicon = c("custom")),\
                                 stop_words)\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 Next, we want to make our data table, doc_article, into a new data table that will have artists in one column and individual words by that artist in another. We do this and call the new data table doc_term\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 doc_term <- doc_article %>%\
  unnest_tokens(word, article)  %>%\
  anti_join(custom_stop_words)\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 If you\'92re wondering what 
\f2 \cb3 %>%
\f0 \cb1  is, it is a convenient way to pull out the first argument of a function, so you don\'92t drown yourself in brackets. This is what the above would look like without 
\f2 \cb3 %>%
\f0 \cb1 \
So, what did we do? 
\f2 \cb3 Unnest_tokens()
\f0 \cb1  takes doc_article and makes all the strings in the article column into individual words. 
\f2 \cb3 anti_join()
\f0 \cb1  gets rid of all individual words that match with any of our custom_stop_words. Next we get rid of all symbols and stem our words. We use two methods, one using 
\f2 \cb3 regex
\f0 \cb1  to get rid of symbols and another using 
\f2 \cb3 grep
\f0 \cb1  to get rid of all terms containing numbers\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 #regex method\
unnest_reg <- "([^A-Za-z_\\\\d#@']|'(?![A-Za-z_\\\\d#@]))"\
doc_term <- doc_term %>%\
  unnest_tokens(word, word, token = "regex", pattern = unnest_reg) %>%\
  filter(!word %in% stop_words$word,\
         str_detect(word, "[a-z]"))\
\
#grep method\
toMatch <- c("0","1", "2", "3","4", "5", "6","7", "8", "9")\
matches <- as.data.frame(grep(paste(toMatch,collapse="|"), \
                          doc_term$word, value=TRUE)) \
colnames(matches) <- c("word")\
doc_term <- doc_term %>%\
  anti_join(matches)\cb1 \
\cb3 ## Warning: Column `word` joining character vector and factor, coercing into\
## character vector\cb4 \
\cb3 #make lower case\
doc_term$word <- tolower(doc_term$word)\
\
#stem words\
doc_term <- doc_term %>%\
  mutate(word = wordStem(word))\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 Stemming words takes words like \'93birdies\'94 and \'93birds\'94 and makes them both into \'93bird\'94. This will help the rest of our analysis. How about we take an artist and make a wordcloud of their most common lyrics. (after removing stop words and stemming the words of course)\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 counter <- doc_term %>%\
  group_by(document) %>%\
  count(word)\
counter <- counter[grep("Grateful Dead", counter$document), ]\
wordcloud(counter$word, counter$n, max.words = 100)\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2  Wow, this is fun. Grateful Dead seems to say things along the line of \'93Love my baby home by the river time\'94. Next, we want to make a new column now, frequency. This column will be a numeric representing the number of times a word appears for a document divided by the total words for that document So, of course, we find those two things.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 document_words <- doc_term %>%\
  group_by(document) %>%\
  count(word, sort=TRUE)\
total_words <-doc_term %>%\
  group_by(document) %>%\
  summarise(total=n())\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 Next we join document_words and total_words. Total_words has waaaaay less rows than document_words, so how can we join them. Well, 
\f2 \cb3 left_join()
\f0 \cb1  will duplicate total_words as many times as needed to make up for the lack of rows, as long as the values in the document column of total_words make a basis for the values in the document column of document_words.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3   frequency <- left_join(document_words,total_words) %>%\
    mutate(freq=n/total)\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 Once these are joined, we have a data frame that has columns like this. 1)\'91document\'92, 2)\'91words\'92, 3)\'91total\'92(number of words per document), 4)\'91n\'92(number of times that word appears per document). We create a new column with 
\f2 \cb3 mutate()
\f0 \cb1  which we called freq and is n/total. By most standards this is called the \'91term frequency\'92 and it is what we continue our analysis with\'85 for now. I also want to get rid of any document with less than 1000 words because that is the minimum sample size I am comfortuable could reasonably represent the artist.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3  frequency <- frequency[frequency[,"total"]>1000,]\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 Now that we have frequency of each word, we can make a tfdm frequency document matrix which will have columns as all the documents and rows as each word. The matrix will be full of all the frequencies each word appears in each document, and considering all artists do not use all words in their songs, there will be a lot of 0\'92s.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 tfdm <- frequency %>%\
  select(document, word, freq) %>%\
  spread(document, freq, fill=0)\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 Since this matrix is most likely to be very sparse, lets remove some rows. More specifically lets remove all rows that less than x percent are not zero. We define commonality as x/100 and remove all rows that less than 3% are not zeros. (97% sparse)\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 commonality=0.03\
tfdm <- tfdm[ rowSums(tfdm > 0) >= commonality*ncol(tfdm), ]\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 Now we have a pretty convenient matrix which I call a ter, frequency document matrix. This is very similar to a document term matrix, something used widely in statistical text analysis. So, lets make a function. I want to find how similar two documents are to eachother. It is as simple as taking a dot product between two document columns. We also need to normalize this number, by adding together the two doucument columns dotted with themselves, and taking the square root of this number. We will make our function called 
\f2 \cb3 cos_sim()
\f0 \cb1 \
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 cosine_sim <- function(document1,document2,tfdm)\{\
    X <- tfdm[[document1]]\
    Y <- tfdm[[document2]]\
    cos <-sum(X*Y)/sqrt((sum(X^2))*(sum(Y^2)))\
    return(cos)\
  \}\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 Now we can find the similarity between two artists. Lets see an example\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 cosine_sim("Johnny Cash","Grateful Dead",tfdm)\cb1 \
\cb3 ## [1] 0.8179433\cb4 \
\cb3 cosine_sim("Johnny Cash","Insane Clown Posse",tfdm)\cb1 \
\cb3 ## [1] 0.4433873\cb4 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 \cb1 Surprise surprise, Johnny Cash is more similar to the Grateful Dead than he is to the Insane Clown Posse. This verifies that our method isn\'92t entirely random, not that it works well. There are far to many factors that that could attribute to lyrics used in a song. However, lets keep going with this method none the less. Lets make a matrix that has columns and rows labled as the documents and full the results of 
\f2 \cb3 cos_sim()
\f0 \cb1  for corresonding artists for that position in the matrix. We will only have to fill this matrix up in the upper triangular. We will repeat this upper triangular in the lower triangular by adding the transverse. There will be an identity matrix of 0\'92s through our matrix as well.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 sim_matrix <- function(tfdm)\{\
    documents <- colnames(tfdm)[-1]\
    n <- length(documents)\
    sim <- matrix(0,n,n)\
    for (i in 1:(n-1))\{\
      for (j in (i+1):n)\{\
        sim[i,j] <- cosine_sim(documents[i],documents[j],tfdm)\
      \}\
    \}\
    colnames(sim) <- colnames(tfdm)[-1]\
    rownames(sim) <- colnames(tfdm)[-1]\
    sim <- sim+t(sim)\
    return(sim)\
\}\
sim <- sim_matrix(tfdm)\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 Now we have our document vs document similarity matrix which we simply call sim. Lets see what the most similar artists are.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 heatmaply(sim ,k_col = 10, k_row = 10,\
          labRow = NA,labCol = NA,\
          colors = Reds(5)) %>%\
  layout(margin = list(l = 20, b = 20))\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 This function uses a k clustering method to cluster similar artists  along an axis.\
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 \cb1 \
Play around with this plot a bit, explore the clusters in the bottom left. It has worked pretty well but I\'92d like to go back and settle a score with one of our methods we used. Lets take a trip back up this case study. Go back to the frequency data table we made that shows how frequent words are for each document and take a look and see what terms appear most.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 frequency  %>%\
  arrange(desc(freq))\cb1 \
\cb3 ## # A tibble: 927,230 x 5\
## # Groups:   document [602]\
##           document  word     n total       freq\
##              <chr> <chr> <int> <int>      <dbl>\
##  1       Sam Smith    la   145  1312 0.11051829\
##  2 Luther Vandross  love  1128 10254 0.11000585\
##  3       Sam Smith    na   119  1312 0.09070122\
##  4  Wilson Pickett    na   135  1622 0.08323058\
##  5        Don Moen  lord   182  2222 0.08190819\
##  6      Diana Ross  love   848 10467 0.08101653\
##  7     Rick Astley  love   238  2951 0.08065063\
##  8 Ten Years After  babi   169  2121 0.07967940\
##  9   Gloria Gaynor  love   259  3623 0.07148772\
## 10     Patsy Cline  love   252  3535 0.07128713\
## # ... with 927,220 more rows\cb4 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 \cb1 It looks like the most frequent word is \'91la\'92 by Sam Smith. But also we see Sam Smith only has a total of 145 words. What if Sam Smith just sang la in a few songs and that happened to be his top few songs and also the only one for Zed in this data set? This is one problem that will lead us to explore other ways. So remember way back when, when we made a new column of our data frame called \'91freq\'92 and I said this was the \'91term frequency\'92, or the number of words per document divided by the total words per document. It turns out this is a pretty archaic way to understand how words appear in our documents. Wikapedia is a great resource for different ways to analyze text, {\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"}}{\fldrslt \cf5 \ul \ulc5 \strokec5 https://en.wikipedia.org/wiki/Tf\'96idf}}, check in the definition section.\uc0\u8232 Once you\'92ve read your fill keep reading. We are going to try this again but instead of using term frequency, we are going to use term frequency inverse document frequency (tf_idf). The tidyverse has a function 
\f2 \cb3 bind_tf_idf
\f0 \cb1 , which can take our document_words data frame from earlier and turn it into a tf_idf for us.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 tf_idf <- document_words %>% bind_tf_idf(word,document,n)\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 Now lets see which words have the highest term frequency times inverse document frequency.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 tf_idf <- tf_idf %>%\
   arrange(desc(tf_idf))\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 I am liking this method already because we no longer have common words like \'91love\'92 that will determine how similar documents are to eachother. It is the strange words that are uncommon and appearing multiple times in one document that dominate similarity now. I never thought I\'92d want to lose the love, but I guess it happens to the best of us. Lets make a term frequency times inverse document frequency document matrix (tf_idfm), in the same way we made a tfdm. Also lets get rid of more than 97% sparse rows.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 tf_idfdm <- tf_idf %>%\
  select(document, word, tf_idf) %>%\
  spread(document, tf_idf, fill=0)\
commonality=0.03\
tf_idfdm <- tf_idfdm[ rowSums(tf_idfdm > 0) >= commonality*ncol(tf_idfdm), ]\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 ``` Now for our patented Johnny Cash/Grateful Dead/Insane Clown Posse verification step\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 cosine_sim("Johnny Cash","Grateful Dead",tf_idfdm)\cb1 \
\cb3 ## [1] 0.2710428\cb4 \
\cb3 cosine_sim("Johnny Cash","Insane Clown Posse",tf_idfdm)\cb1 \
\cb3 ## [1] 0.1171236\cb4 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 \cb1 Seems to work well, although not as big a difference between these as our last method. Next, literally repeating our previous analysis we make a sim matrix and apply 
\f2 \cb3 heatmaply
\f0 \cb1 .\
Now we have our document vs document similarity matrix which we simply call sim. Lets see what the most similar artists are.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 sim <- sim_matrix(tf_idfdm)\
heatmaply(sim ,k_col = 10, k_row = 10,\
          labRow = NA,labCol = NA,\
          colors = Reds(5)) %>%\
  layout(margin = list(l = 20, b = 20))\cb1 \
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0 \cf2 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
We see rappers in the bottom left, and christian music in the top right.\
\pard\pardeftab720\sl280\sa240\partightenfactor0
\cf2 \
Here is a function that will take a document article data frame and turn it into a tf_idfdm.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf2 \cb3 make_tf_idfm <- function(doc_article, custom_stop_words=c(""), commonality=0.03)\{\
\
  custom_stop_words <- bind_rows(data_frame(word = custom_stop_words,\
                                            lexicon = c("custom")),\
                                 stop_words)\
                                 \
  doc_term <- doc_article %>%\
    unnest_tokens(word, article)  %>%\
    anti_join(custom_stop_words)\
\
  unnest_reg <- "([^A-Za-z_\\\\d#@']|'(?![A-Za-z_\\\\d#@]))"\
\
  doc_term <- doc_term %>%\
    unnest_tokens(word, word, token = "regex", pattern = unnest_reg) %>%\
    filter(!word %in% stop_words$word,\
           str_detect(word, "[a-z]"))\
\
  toMatch <- c("0","1", "2", "3","4", "5", "6","7", "8", "9")\
  matches <- as.data.frame(grep(paste(toMatch,collapse="|"),\
                          doc_term$word, value=TRUE))\
  colnames(matches) <- c("word")\
  doc_term %>%\
    anti_join(matches)\
\
  doc_term$word <- tolower(doc_term$word)\
\
\
  doc_term <- doc_term %>%\
    mutate(word = wordStem(word))\
\
  document_words <- doc_term %>%\
    group_by(document) %>%\
    count(word, sort=TRUE)\
  tf_idf <- document_words %>% bind_tf_idf(word,document,n)\
\
  tf_idfdm <- tf_idf %>%\
    select(document, word, tf_idf) %>%\
    spread(document, tf_idf, fill=0)\
  tf_idfdm <- tf_idfdm[ rowSums(tf_idfdm > 0) >= commonality*ncol(tf_idfdm), ]\
  \
  return(tf_idfdm)\
\}\cb1 \
}